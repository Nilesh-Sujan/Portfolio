{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzUUsYRUWwPG"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en7oPrJfW7WM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = 'kyriacospanas'  # Your Kaggle username\n",
        "os.environ['KAGGLE_KEY'] = '1bbf35726a612dad5d46ed41bcbb5a4f'  # Your Kaggle API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86aurC77W9ko",
        "outputId": "5112b5af-652d-4d26-918d-90869a5c5112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading skin-cancer-mnist-ham10000.zip to /content\n",
            "100% 5.20G/5.20G [03:49<00:00, 24.6MB/s]\n",
            "100% 5.20G/5.20G [03:49<00:00, 24.3MB/s]\n",
            "Downloading ham10000-test.zip to /content\n",
            "100% 400M/401M [00:18<00:00, 25.4MB/s]\n",
            "100% 401M/401M [00:18<00:00, 23.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000\n",
        "!kaggle datasets download -d kyriacospanas/ham10000-test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XRF0Kl8GWmt"
      },
      "outputs": [],
      "source": [
        "!unzip -q ham10000-test.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhMQgG5MW-no"
      },
      "outputs": [],
      "source": [
        "!unzip -q skin-cancer-mnist-ham10000.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttbdbtSzdXfs"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Replace 'my_directory' with the name of the directory you want to delete\n",
        "dir_path = '/content/ham10000_images_part_1'\n",
        "\n",
        "shutil.rmtree(dir_path)\n",
        "\n",
        "dir_path = '/content/ham10000_images_part_2'\n",
        "\n",
        "shutil.rmtree(dir_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lXfS4DUgsDa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "training_data_dir = '../content'\n",
        "\n",
        "all_image_path = []\n",
        "for root, dirs, files in os.walk(training_data_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.jpg'):\n",
        "            all_image_path.append(os.path.join(root, file))\n",
        "\n",
        "imageID_path_dict = {}\n",
        "for path in all_image_path:\n",
        "    filename = os.path.splitext(os.path.basename(path))[0]\n",
        "    imageID_path_dict[filename] = path\n",
        "\n",
        "lesion_type_dict = {\n",
        "    'nv': 'Melanocytic nevi',\n",
        "    'mel': 'melanoma',\n",
        "    'bkl': 'Benign keratosis-like lesions ',\n",
        "    'bcc': 'Basal cell carcinoma',\n",
        "    'akiec': 'Actinic keratoses',\n",
        "    'vasc': 'Vascular lesions',\n",
        "    'df': 'Dermatofibroma'\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lesion_type_dict_capital = {\n",
        "    'NV': 'Melanocytic nevi',\n",
        "    'MEL': 'melanoma',\n",
        "    'BKL': 'Benign keratosis-like lesions ',\n",
        "    'BCC': 'Basal cell carcinoma',\n",
        "    'AKIEC': 'Actinic keratoses',\n",
        "    'VASC': 'Vascular lesions',\n",
        "    'DF': 'Dermatofibroma'\n",
        "}"
      ],
      "metadata": {
        "id": "Q74K_bg0UiwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzrcANNWo_6D"
      },
      "outputs": [],
      "source": [
        "import os, cv2,itertools\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def calculate_image_mean_std(image_paths):\n",
        "\n",
        "    image_height, image_width = 224, 224\n",
        "    images = []\n",
        "    means = []\n",
        "    stdevs = []\n",
        "\n",
        "    for i in tqdm(range(len(image_paths))):\n",
        "        image = cv2.imread(image_paths[i])\n",
        "        image = cv2.resize(image, (image_height, image_width))\n",
        "        images.append(image)\n",
        "\n",
        "    images = np.stack(images, axis=3)\n",
        "    print(images.shape)\n",
        "\n",
        "    images = images.astype(np.float32) / 255.\n",
        "\n",
        "    for i in range(3):\n",
        "        pixels = images[:, :, i, :].ravel()\n",
        "        mean = np.mean(pixels)\n",
        "        std = np.std(pixels)\n",
        "        means.append(mean)\n",
        "        stdevs.append(std)\n",
        "\n",
        "    means.reverse()\n",
        "    stdevs.reverse()\n",
        "\n",
        "    print(\"normalised Mean values = {}\".format(means))\n",
        "    print(\"normalised Std values = {}\".format(stdevs))\n",
        "\n",
        "    return means,stdevs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxr6DdhJGu9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c02864fb-56a3-4f5a-bd53-0286f5a201cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11527/11527 [01:51<00:00, 103.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(224, 224, 3, 11527)\n"
          ]
        }
      ],
      "source": [
        "norm_mean,norm_std = calculate_image_mean_std(all_image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYZriimjwjeC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(os.path.join('/content', 'HAM10000_metadata.csv'))\n",
        "\n",
        "df['file_path'] = df['image_id'].map(imageID_path_dict.get)\n",
        "df['cell_type'] = df['dx'].map(lesion_type_dict.get)\n",
        "df['cell_type_idx'] = pd.Categorical(df['cell_type']).codes\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMCUbKXf-8E5"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2qQFm-T_zSd"
      },
      "outputs": [],
      "source": [
        "df['age'].fillna(int(df['age'].mean()),inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJBwzkla_2Bx"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NW-tfry0RWX2"
      },
      "outputs": [],
      "source": [
        "df['cell_type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX9Yl6pkUFn-"
      },
      "outputs": [],
      "source": [
        "df['cell_type_idx'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3m3KlzAXIcz"
      },
      "outputs": [],
      "source": [
        "df_balanced = df\n",
        "\n",
        "# Copy fewer class to balance the number of 7 classes\n",
        "data_aug_rate = [15,10,5,55,0,45,5]\n",
        "for i in range(7):\n",
        "    if data_aug_rate[i]:\n",
        "        df_balanced=df_balanced.append([df.loc[df['cell_type_idx'] == i,:]]*(data_aug_rate[i]-1), ignore_index=True)\n",
        "df_balanced['cell_type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V498H4sA3a4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameter\n",
        "batch_size = 128\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39adqJz_ArRZ"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torch.nn as nn\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, 7).to(device)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knfjURorDR9E"
      },
      "outputs": [],
      "source": [
        "from torchvision import models,transforms\n",
        "# norm_mean = (0.49139968, 0.48215827, 0.44653124)\n",
        "# norm_std = (0.24703233, 0.24348505, 0.26158768)\n",
        "# define the transformation of the train images.\n",
        "train_transform = transforms.Compose([transforms.Resize((224,224)),transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomVerticalFlip(),transforms.RandomRotation(20),\n",
        "                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n",
        "                                        transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)])\n",
        "\n",
        "val_transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(),\n",
        "                                    transforms.Normalize(norm_mean, norm_std)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-wPhrTJFsbj"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader,Dataset\n",
        "from PIL import Image\n",
        "\n",
        "# Define a pytorch dataloader for this dataset\n",
        "class HAM10000(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load data and get label\n",
        "        X = Image.open(self.df['file_path'][index])\n",
        "        y = torch.tensor(int(self.df['cell_type_idx'][index]))\n",
        "\n",
        "        if self.transform:\n",
        "            X = self.transform(X)\n",
        "\n",
        "        return (X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h7YX4cCLd0_"
      },
      "outputs": [],
      "source": [
        "from torch import optim,nn\n",
        "\n",
        "# we use Adam optimizer, use cross entropy loss as our loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKkgmYxzQ8GM"
      },
      "outputs": [],
      "source": [
        "df_balanced.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yJc4zqxQ0Lj"
      },
      "outputs": [],
      "source": [
        "num_cells = df_balanced.size\n",
        "print(\"Number of cells in df_balanced:\", num_cells)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXkdXxeOJTKM"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLRi0-BUJV5p"
      },
      "outputs": [],
      "source": [
        "def validate(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUksWoscnedF"
      },
      "outputs": [],
      "source": [
        "df_balanced.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3jgz7JsIVck"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from torch import optim,nn\n",
        "\n",
        "# Define the number of folds\n",
        "num_folds = 2\n",
        "\n",
        "# Use KFold from scikit-learn to split the data into k-folds\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# Loop over the k-folds\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(df_balanced)):\n",
        "    # Create training and validation sets for this fold\n",
        "    train_set = df_balanced.iloc[train_idx].reset_index(drop=True)\n",
        "    val_set = df_balanced.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "    print(\"Length of Df:\", len(df_balanced))\n",
        "    print(\"Length of train_set:\", len(train_set))\n",
        "    print(\"Length of val_set:\", len(val_set))\n",
        "\n",
        "    # Create data loaders for this fold\n",
        "    training_set = HAM10000(train_set, transform=train_transform)\n",
        "    train_loader = DataLoader(training_set, batch_size=32, shuffle=True)\n",
        "\n",
        "    validation_set = HAM10000(val_set, transform=val_transform)\n",
        "    val_loader = DataLoader(validation_set, batch_size=32, shuffle=False)\n",
        "\n",
        "    # train your model on the current fold\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    print(\"Fold:\", fold + 1)\n",
        "    epochs = 2\n",
        "    for t in range(epochs):\n",
        "      print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "      train(train_loader, model, criterion, optimizer)\n",
        "      validate(val_loader, model, criterion)\n",
        "      print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "MOQqWwVYzQHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a pytorch dataloader for this dataset\n",
        "class SkinTestDf(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load data and get label\n",
        "        X = Image.open(self.df['file_path'][index])\n",
        "        y = torch.tensor(int(self.df['cell_type_idx'][index]))\n",
        "\n",
        "        if self.transform:\n",
        "            X = self.transform(X)\n",
        "\n",
        "        return X, y"
      ],
      "metadata": {
        "id": "JcFHsV26Xe3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cell_type_idx = []\n",
        "dfTest = pd.read_csv('ISIC2018_Task3_Test_GroundTruth.csv')\n",
        "for index, row in dfTest.iterrows():\n",
        "    # Access values by column name\n",
        "    if 1 in row.values:\n",
        "      columns_with_1 = row[row == 1].index.tolist()\n",
        "      dfTest.at[index, 'columns_with_1'] = ', '.join(columns_with_1)\n",
        "      dfTest['file_path'] = dfTest['image'].map(imageID_path_dict.get)\n",
        "      dfTest['cell_type'] = dfTest['columns_with_1'].map(lesion_type_dict_capital.get)\n",
        "      dfTest['cell_type_idx'] = pd.Categorical(dfTest['cell_type']).codes\n"
      ],
      "metadata": {
        "id": "YfoZr8CTH6Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfTest.head()"
      ],
      "metadata": {
        "id": "psP3GgBvIHtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbkjBZDqI4T-"
      },
      "outputs": [],
      "source": [
        "test_transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(),transforms.Normalize(norm_mean, norm_std)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlAeEwLeJFBl"
      },
      "outputs": [],
      "source": [
        "test_dataset = SkinTestDf(dfTest, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "model.eval()\n",
        "y_label = []\n",
        "y_predict = []\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, prediction = torch.max(outputs, 1)\n",
        "        y_label.extend([labels.item()])\n",
        "        y_predict.extend(prediction.cpu().numpy())\n",
        "\n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(y_label, y_predict)\n",
        "# plot the confusion matrix\n",
        "plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc','mel']\n",
        "plot_confusion_matrix(confusion_mtx, plot_labels)"
      ],
      "metadata": {
        "id": "7KSsEgo10JaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# Generate a classification report\n",
        "report = classification_report(y_label, y_predict, target_names=plot_labels)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "lGgLOqO9L4Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_frac_error = 1 - np.diag(confusion_mtx) / np.sum(confusion_mtx, axis=1)\n",
        "plt.bar(np.arange(7),label_frac_error)\n",
        "plt.xlabel('True Label')\n",
        "plt.ylabel('Fraction classified incorrectly')"
      ],
      "metadata": {
        "id": "vqqhCBXUMB_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}